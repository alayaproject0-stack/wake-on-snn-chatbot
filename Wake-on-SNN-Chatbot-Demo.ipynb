{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Wake-on-SNN Ã— BERT Hybrid Chatbot (Colab Demo)",
                "",
                "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€ã€ŒWake-on-SNN Ã— BERT Hybrid Chatbotã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’Google Colabç’°å¢ƒã§å®Ÿè¡Œã™ã‚‹ãŸã‚ã®ãƒ‡ãƒ¢ã§ã™ã€‚",
                "",
                "---",
                "",
                "## 1. ç’°å¢ƒæ§‹ç¯‰ã¨ãƒ•ã‚¡ã‚¤ãƒ«ã®æº–å‚™",
                "",
                "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã€GitHubãƒªãƒã‚¸ãƒˆãƒªã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¯ãƒ­ãƒ¼ãƒ³ã—ã¾ã™ã€‚"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«",
                "!pip install torch transformers scikit-learn",
                "",
                "# GitHubãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³",
                "!git clone https://github.com/alayaproject0-stack/wake-on-snn-chatbot.git",
                "",
                "# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç§»å‹•",
                "import os",
                "os.chdir('wake-on-snn-chatbot')",
                "",
                "# ãƒ•ã‚¡ã‚¤ãƒ«æ§‹æˆã®ç¢ºèª",
                "!ls"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---",
                "",
                "## 2. ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å®šç¾©ã¨åˆæœŸåŒ–",
                "",
                "`run_chat.py`ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’Colabã§å®Ÿè¡Œã™ã‚‹ãŸã‚ã«ã€å¿…è¦ãªã‚¯ãƒ©ã‚¹å®šç¾©ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã¾ãŸã¯å†å®šç¾©ã—ã¾ã™ã€‚",
                "",
                "**æ³¨æ„:** Colabç’°å¢ƒã§å¯¾è©±çš„ã«å®Ÿè¡Œã™ã‚‹ãŸã‚ã€`run_chat.py`ã®`input()`ãƒ«ãƒ¼ãƒ—ã¯æ¬¡ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§åˆ¥é€”å®Ÿè£…ã—ã¾ã™ã€‚"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json",
                "import torch",
                "import numpy as np",
                "from transformers import pipeline",
                "",
                "# --- wake_controller.py ã®å†…å®¹ ---",
                "class WakeController:",
                "    def __init__(self, thr):",
                "        self.thr = thr",
                "",
                "    def should_wake(self, confidence):",
                "        return confidence < self.thr",
                "",
                "# --- snn_model.py ã®å†…å®¹ (ãƒ€ãƒŸãƒ¼å®Ÿè£…) ---",
                "class DummyModel:",
                "    def to(self, device): return self",
                "    def eval(self): return self",
                "    def __call__(self, x):",
                "        return torch.tensor([[0.5, 0.5]]), None",
                "",
                "class DummyVectorizer:",
                "    def transform(self, text_list):",
                "        return np.array([[0.1, 0.2]])",
                "",
                "class SNNGate:",
                "    def __init__(self, snn_model, vectorizer, device):",
                "        self.model = snn_model",
                "        self.vec = vectorizer",
                "        self.device = device",
                "",
                "    @torch.no_grad()",
                "    def confidence(self, text):",
                "        # ãƒ€ãƒŸãƒ¼å®Ÿè£…: ãƒ†ã‚­ã‚¹ãƒˆã®é•·ã•ã«å¿œã˜ã¦confidenceã‚’è¿”ã™",
                "        length_score = len(text.split()) / 10.0",
                "        confidence_score = max(0.1, 0.8 - length_score)",
                "        return confidence_score",
                "",
                "# --- bert_model.py ã®å†…å®¹ (ãƒ€ãƒŸãƒ¼å®Ÿè£…) ---",
                "class BertResponder:",
                "    def __init__(self):",
                "        # å®Ÿéš›ã«ã¯ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’åˆæœŸåŒ–ã™ã‚‹ãŒã€ã“ã“ã§ã¯ãƒ€ãƒŸãƒ¼",
                "        pass",
                "",
                "    def respond(self, text):",
                "        # ãƒ€ãƒŸãƒ¼å®Ÿè£…: ãƒ†ã‚­ã‚¹ãƒˆã®é•·ã•ã«å¿œã˜ã¦ãƒ©ãƒ™ãƒ«ã‚’è¿”ã™",
                "        if len(text) > 15:",
                "            label = \"å“²å­¦\"",
                "        elif len(text) > 8:",
                "            label = \"æŠ€è¡“\"",
                "        else:",
                "            label = \"æ—¥å¸¸\"",
                "        return f\"è€ƒãˆã¦ç­”ãˆã‚‹ã¨â€¦ã“ã®è©±é¡Œã¯ã€Œ{label}ã€ã§ã™ã­ã€‚\"",
                "",
                "# --- chatbot.py ã®å†…å®¹ ---",
                "class WakeChatBot:",
                "    def __init__(self, snn_gate, wake_ctrl):",
                "        self.snn = snn_gate",
                "        self.wake = wake_ctrl",
                "        self.bert = BertResponder()",
                "",
                "        with open(\"responses.json\", \"r\", encoding=\"utf-8\") as f:",
                "            self.light = json.load(f)",
                "",
                "    def reply(self, text):",
                "        conf = self.snn.confidence(text)",
                "",
                "        if not self.wake.should_wake(conf):",
                "            # è»½é‡å¿œç­”ã®æ¤œç´¢",
                "            for k, v in self.light.items():",
                "                if k in text.lower():",
                "                    return f\"[SNN] {v}\"",
                "            return \"[SNN] ãªã‚‹ã»ã©\"",
                "",
                "        return \"[BERT] \" + self.bert.respond(text)",
                "",
                "# --- run_chat.py ã®åˆæœŸåŒ–éƒ¨åˆ† ---",
                "snn_model = DummyModel()",
                "vectorizer = DummyVectorizer()",
                "",
                "# Colabã§ã¯GPUãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆãŒã‚ã‚‹ãŸã‚ã€è‡ªå‹•ã§ãƒ‡ãƒã‚¤ã‚¹ã‚’é¸æŠ",
                "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"",
                "",
                "gate = SNNGate(snn_model, vectorizer, device) ",
                "wake = WakeController(thr=0.434)  # wake_q=0.30ç”±æ¥",
                "",
                "bot = WakeChatBot(gate, wake)",
                "",
                "print(f\"Chatbot initialized on device: {device}\")",
                "print(\"Wake-on-SNN Chatbot æº–å‚™å®Œäº†\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---",
                "",
                "## 3. ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®å®Ÿè¡Œ",
                "",
                "ä»¥ä¸‹ã®ã‚»ãƒ«ã‚’å®Ÿè¡Œã—ã€`You >` ã®å¾Œã«è³ªå•ã‚’å…¥åŠ›ã—ã¦Enterã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Colabã§ã®å¯¾è©±å®Ÿè¡Œç”¨é–¢æ•°",
                "def run_interactive_chat(chatbot):",
                "    print(\"--- Wake-on-SNN Chatbot èµ·å‹• (exitã§çµ‚äº†) ---\")",
                "    while True:",
                "        try:",
                "            text = input(\"You> \")",
                "            if text.lower() == \"exit\":",
                "                break",
                "            if not text.strip():",
                "                continue",
                "            print(\"Bot>\", chatbot.reply(text))",
                "        except EOFError:",
                "            print(\"\\nExiting chat loop.\")",
                "            break",
                "        except KeyboardInterrupt:",
                "            print(\"\\nExiting chat loop.\")",
                "            break",
                "",
                "# å®Ÿè¡Œ",
                "run_interactive_chat(bot)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---",
                "",
                "## 4. å®Ÿè¡Œä¾‹ã®ç¢ºèª",
                "",
                "ä»¥ä¸‹ã®å…¥åŠ›ä¾‹ã‚’è©¦ã—ã¦ã€SNNã¨BERTã®åˆ‡ã‚Šæ›¿ãˆã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚",
                "",
                "| å…¥åŠ›ä¾‹ | æœŸå¾…ã•ã‚Œã‚‹å‹•ä½œ |",
                "| :--- | :--- |",
                "| `hello` | `[SNN] ã“ã‚“ã«ã¡ã¯ğŸ˜Š` (è»½é‡å¿œç­”) |",
                "| `ã‚ã‚ŠãŒã¨ã†` | `[SNN] ãªã‚‹ã»ã©` (è»½é‡å¿œç­”ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯) |",
                "| `äººé–“ã¯ãªãœäº‰ã†ã®ï¼Ÿ` | `[BERT] è€ƒãˆã¦ç­”ãˆã‚‹ã¨â€¦ã“ã®è©±é¡Œã¯ã€Œå“²å­¦ã€ã§ã™ã­ã€‚` (BERTèµ·å‹•) |",
                "| `AIã®å€«ç†ã«ã¤ã„ã¦æ•™ãˆã¦` | `[BERT] è€ƒãˆã¦ç­”ãˆã‚‹ã¨â€¦ã“ã®è©±é¡Œã¯ã€ŒæŠ€è¡“ã€ã§ã™ã­ã€‚` (BERTèµ·å‹•) |"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}